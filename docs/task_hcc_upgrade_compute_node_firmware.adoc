---
sidebar: docs_sidebar
permalink: docs/task_hcc_upgrade_compute_node_firmware.html
summary: As part of a NetApp HCI system upgrade, you upgrade compute node firmware.
keywords: netapp, hci, on premise, cluster, element, compute node
---

= Upgrade compute node firmware

:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

[.lead]
For any H-series compute node, you can upgrade the firmware for hardware components such as the BMC, BIOS, and NIC. To upgrade compute node firmware, you can use the NetApp Hybrid Cloud Control UI, REST API, a USB drive with the latest firmware image, or the BMC UI.

After the upgrade, the compute node boots into ESXi and works as before, retaining the configuration.


.What you'll need

* For USB and BMC UI methods or to see a list of available upgrades for your node model, see the firmware and driver matrix for your hardware in https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/Firmware_and_driver_versions_in_NetApp_HCI_and_NetApp_Element_software[this KB article] (login required).
* *Admin privileges*: You have cluster administrator permissions to perform the upgrade.


.About this task

In production environments, upgrade the firmware on one compute node at a time.

For Hybrid Cloud Control UI or API upgrades, your ESXi host will be automatically placed in maintenance mode during the upgrade process. Once the upgrade process is complete, the ESXi host will be taken out of maintenance mode. For USB and BMC UI options, you will need to place the ESXi host in maintenance mode manually, as described in each procedure.

.Upgrade options

Choose the option that is relevant to your upgrade scenario:

* <<Use NetApp Hybrid Cloud Control UI to upgrade a compute node>>
* <<Use NetApp Hybrid Cloud Control API to upgrade a compute node>>
* For compute node image 12.0: <<Use a USB drive with the latest firmware image downloaded>>
* For compute firmware 12.2.92: <<Use the Baseboard Management Controller (BMC) user interface (UI)>>

+
TIP: It takes approximately 25 to 30 minutes for the upgrade via the BMC UI.

== Use NetApp Hybrid Cloud Control UI to upgrade a compute node

The firmware on a compute node should only be upgraded if:
the compute node is in maintenance mode in order to ensure the upgrade does not interfere with any active workloads. (Service / API driven)
the health check on the vSphere cluster is successful to ensure the upgrade has the greatest chance to succeed.

== Use NetApp Hybrid Cloud Control API to upgrade a compute node

You can use APIs to upgrade compute nodes in a cluster to the latest firmware version. You can use an automation tool of your choice to run the APIs. The API workflow documented here uses the REST API UI available on the management node as an example.

.Steps

. Do one of the following depending on your connection:
+
[%header,cols=2*]
|===
|Option
|Steps

|Your management node has external connectivity.
a|
. Verify the repository connection:
.. Open the package service REST API UI on the management node:
+
----
https://[management node IP]/package-repository/1/
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client`.
... Click *Authorize* to begin a session.
... Close the authorization window.
.. From the REST API UI, click *GET ​/packages​/remote-repository​/connection*.
.. Click *Try it out*.
.. Click *Execute*.
.. If code 200 is returned, go to the next step. If there is no connection to the remote repository, establish the connection or use the dark site option.
. Find the upgrade package ID:
.. From the REST API UI, click *GET /packages*.
.. Click *Try it out*.
.. Click *Execute*.
.. From the response, copy and save the package ID for use in a later step.

|Your management node is within a dark site without external connectivity.
a|
. Go to the NetApp HCI software https://mysupport.netapp.com/site/products/all/details/netapp-hci/downloads-tab[download page] and download the latest compute node firmware image to a device that is accessible to the management node.
. Upload the compute firmware upgrade package to the management node:
.. Open the management node REST API UI on the management node:
+
----
https://[management node IP]/package-repository/1/
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client`.
... Click *Authorize* to begin a session.
... Close the authorization window.
.. From the REST API UI, click *POST /packages*.
.. Click *Try it out*.
.. Click *Browse* and select the upgrade package.
.. Click *Execute* to initiate the upload.
.. From the response, copy and save the package ID (`"id"`) for use in a later step.
. Verify the status of the upload.
.. From the REST API UI, click *GET​ /packages​/{id}​/status*.
.. Click *Try it out*.
.. Enter the package ID you copied in the previous step in *id*.
.. Click *Execute* to initiate the status request.
+
The response indicates `state` as `finished` when complete.
|===
. Locate the compute controller ID and node hardware tag for the node you intend to upgrade:
.. Open the management node REST API UI on the management node:
+
----
https://[management node IP]/inventory
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client`.
... Click *Authorize* to begin a session.
... Close the authorization window.
.. From the REST API UI, click *GET /installations*.
.. Click *Try it out*.
.. Click *Execute*.
.. From the response, copy the installation asset ID (`"id"`).
.. From the REST API UI, click *GET /installations/{id}*.
.. Click *Try it out*.
.. Paste the installation asset ID into the *id* field.
.. Click *Execute*.
.. From the response, copy and save the cluster controller ID (`"controllerId"`) and node hardware tag (`"hardwareTag"`) for use in a later step.
. Run the compute node firmware upgrade:
.. Open the hardware service REST API UI on the management node:
+
----
https://[management node IP]/hardware
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client`.
... Click *Authorize* to begin a session.
... Close the authorization window.
.. Click *POST /nodes/{hardware_id}/upgrades*.
.. Click *Try it out*.
.. Enter the hardware host asset ID (`"hardwareTag"` saved a previous step) in the parameter field.
.. Do the following to the payload values:
... Retain the values `"force": false` and `"maintenanceMode": true"` so that health checks are performed on the node and the ESXi host is set to maintenance mode.
... Enter the cluster controller ID (`"controllerId"` saved a the previous step).
... Enter the package name and package version indicated on the bundle you downloaded.
+
----
{
  "config": {
    "force": false,
    "maintenanceMode": true
  },
  "controllerId": "00000000-0000-0000-0000-AC1F6BC4ECF6",
  "packageName": "compute_firmware",
  "packageVersion": "12.2.92"
}
----

.. Click *Execute* to initiate the upgrade.
.. Copy the upgrade task ID (`"taskId"`) that is part of the response.
. Verify the upgrade progress and results:
.. Click *GET /task/{task_id}/logs*.
.. Click *Try it out*.
.. Enter the task ID from the previous step in *task_Id*.
.. Click *Execute*.
.. Do one of the following if there are problems or special requirements during the upgrade:
+
[%header,cols=2*]
|===
|Option
|Steps

|You need to correct cluster health issues due to `failedHealthChecks` message in the response body.
a|
. Go to the specific KB article listed for each issue or perform the specified remedy.
. If a KB is specified, complete the process described in the relevant KB article.
. After you have resolved cluster issues, reauthenticate if needed and click *PUT ​/upgrades/{upgradesId}*.
. Click *Try it out*.
. Enter the upgrade ID from the previous step in *upgradeId*.
. Enter `"action":"resume"` in the request body.
+
----
{
  "action": "resume"
}
----
. Click *Execute*.

|You need to pause the upgrade because the maintenance window is closing or for another reason.
a|
. Reauthenticate if needed and click *PUT ​/upgrades/{upgradesId}*.
. Click *Try it out*.
. Enter the upgrade ID from the previous step in *upgradeId*.
. Enter `"action":"pause"` in the request body.
+
----
{
  "action": "pause"
}
----
. Click *Execute*.

|If you are upgrading an H610S cluster running Element version earlier than 11.8, you see the state `finishedNeedsAck` in the response body. You need to perform additional upgrade steps (phase 2) for each H610S storage node.
a|
. See <<Upgrading H610S storage nodes to Element 12.0 or later (phase 2)>> and complete the process for each node.
. Reauthenticate if needed and click *PUT ​/upgrades/{upgradesId}*.
. Click *Try it out*.
. Enter the upgrade ID from the previous step in *upgradeId*.
. Enter `"action":"acknowledge"` in the request body.
+
----
{
  "action": "acknowledge"
}
----
. Click *Execute*.
|===
.. Run the *GET ​/upgrades/{upgradesId}* API multiple times, as needed, until the process is complete.
+
During the upgrade, the `status` indicates `running` if no errors are encountered. As each node is upgraded, the `step` value changes to `NodeFinished`.
+
The upgrade has finished successfully when the `percent` value is `100` and the `state` indicates `finished`.
. Confirm upgraded firmware versions:
.. Open the hardware service REST API UI on the management node:
+
----
https://[management node IP]/hardware
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client`.
... Click *Authorize* to begin a session.
... Close the authorization window.
.. From the REST API UI, click *GET ​/nodes​/{hardware_id}​/upgrades*.
.. Enter the hardware host asset ID (`"hardwareTag"` saved from a previous step) in the parameter field.
.. Click *Try it out*.
.. Click *Execute*.

== Use a USB drive with the latest firmware image downloaded

You can insert a USB drive with the latest firmware image downloaded into a USB port on the compute node. As an alternative to using the USB thumb drive method described in this procedure, you can mount the compute node RTFI image on the compute node using the *Virtual CD/DVD* option in the Virtual Console in the Baseboard Management Controller (BMC) interface. The BMC method takes considerably longer than the USB thumb drive method. Ensure that your workstation or server has the necessary network bandwidth and that your browser session with the BMC does not time out.

.Steps

. Browse to the https://mysupport.netapp.com/site/downloads[NetApp software downloads] page, click *NetApp HCI*, and click the download link for correct version of NetApp HCI.
. Accept the End User License Agreement.
. Under the *Compute and Storage Nodes* section, download the compute node image.
. Write the raw contents of the compute node RTFI image to a USB thumb drive with at least 32GB capacity (using dd or Etcher).
. Place the compute node in maintenance mode using VMware vCenter, and evacuate all virtual machines from the host.
+
NOTE: If VMware Distributed Resource Scheduler (DRS) is enabled on the cluster (this is the default in NetApp HCI installations), virtual machines will automatically be migrated to other nodes in the cluster.

. Insert the USB thumb drive into a USB port on the compute node and reboot the compute node using VMware vCenter.
. During the compute node POST cycle, press *F11* to open the Boot Manager. You may need to press *F11* multiple times in quick succession. You can perform this operation by connecting a video/keyboard or by using the console in `BMC`.
. Select *One Shot* > *USB Flash Drive* from the menu that appears. If the USB thumb drive does not appear in the menu, verify that USB Flash Drive is part of the legacy boot order in the BIOS of the system.
. Press *Enter* to boot the system from the USB thumb drive. The firmware flash process begins.
+
After firmware flashing is complete and the node reboots, it might take a few minutes for ESXi to start.
. After the reboot is complete, exit maintenance mode on the upgraded compute node using vCenter.
. Remove the USB flash drive from the upgraded compute node.
. Repeat this task for other compute nodes in your ESXi cluster until all compute nodes are upgraded.

== Use the Baseboard Management Controller (BMC) user interface (UI)

You must perform the sequential steps to load the compute firmware ISO and reboot the node to the ISO to ensure that the upgrade is successful. The ISO should be located on the system or virtual machine (VM) hosting the web browser. Ensure that you have downloaded the ISO before you start the process.

TIP: The recommendation is to have the system or VM and the node on the same network.

* <<Upgrade firmware on H410C and H300E/H500E/H700E nodes>>
* <<Upgrade firmware on H610C/H615C nodes>>

=== Upgrade firmware on H410C and H300E/H500E/H700E nodes

If your node is part of a cluster, you must place the node in maintenance mode before the upgrade, and take it out of maintenance mode after the upgrade.

TIP: Ignore the following informational message you see during the process: `Untrusty Debug Firmware Key is used, SecureFlash is currently in Debug Mode`

.Steps

. If your node is part of a cluster, place it in maintenance mode as follows. If not, skip to step 2.
.. Log in to the VMware vCenter web client.
.. Right-click the host (compute node) name and select *Maintenance Mode > Enter Maintenance Mode*.
.. Click *OK*.
VMs on the host will be migrated to another available host. VM migration can take time depending on the number of VMs that need to be migrated.
+
CAUTION: Ensure that all the VMs on the host are migrated before you proceed.

. Navigate to the BMC UI, `https://BMCIP/#login`, where BMCIP is the IP address of the BMC.
. Log in using your credentials.
. Select *Remote Control > Console Redirection*.
. Click *Launch Console*.
+
NOTE: You might have to install Java or update it.

. When the console opens, click *Virtual Media > Virtual Storage*.
. On the *Virtual Storage* screen, click *Logical Drive Type*, and select *ISO File*.
+
image:BIOS_H410C_iso.png[Shows the navigation path to select the ISO file.]

. Click *Open Image* to browse to the folder where you downloaded the ISO file, and select the ISO file.
. Click *Plug In*.
. When the connection status shows `Device#: VM Plug-in OK!!`, click *OK*.
. Reboot the node by pressing *F12* and clicking *Restart* or clicking *Power Control > Set Power Reset*.
. During reboot, press *F11* to select the boot options and load the ISO. You might have to press F11 a few times before the boot menu is displayed.
+
You will see the following screen:
+
image:boot_option_iso_h410c.png[Shows the screen the virtual ISO boots up to.]

. On the above screen, press *Enter*. Depending on your network, it might take a few minutes after you press *Enter* for the upgrade to begin.
+
NOTE: NOTE: Some of the firmware upgrades might cause the console to disconnect and/or cause your session on the BMC to disconnect. You can log back into the BMC, however some services, such as the console, may not be available due to the firmware upgrades. After the upgrades have completed, the node will perform a cold reboot, which can take approximately five minutes.

. Log back in to the BMC UI and click *System* to verify the BIOS version and build time after booting to the OS. If the upgrade completed correctly, you see the new BIOS and BMC versions.
+
NOTE: The BIOS version will not show the upgraded version until the node has finished fully booting.

. If the node is part of a cluster, complete the steps below. If it is a standalone node, no further action is needed.
.. Log in to the VMware vCenter web client.
.. Take the host out of maintenance mode. This might show a disconnected red flag. Wait until all statuses are cleared.
.. Power on any of the remaining VMs that were powered off.

=== Upgrade firmware on H610C/H615C nodes

The steps vary depending on whether the node is standalone or part of a cluster. The procedure can take approximately 25 minutes and includes powering the node off, uploading the ISO, flashing the devices, and powering the node back on after the upgrade.

.Steps

. If your node is part of a cluster, place it in maintenance mode as follows. If not, skip to step 2.
.. Log in to the VMware vCenter web client.
.. Right-click the host (compute node) name and select *Maintenance Mode > Enter Maintenance Mode*.
.. Click *OK*.
VMs on the host will be migrated to another available host. VM migration can take time depending on the number of VMs that need to be migrated.
+
CAUTION: Ensure that all the VMs on the host are migrated before you proceed.

. Navigate to the BMC UI, `https://BMCIP/#login`, where BMC IP is the IP address of the BMC.
. Log in using your credentials.
. Click *Remote Control > Launch KVM (Java)*.
. In the console window, click *Media > Virtual Media Wizard*.
+
image::bmc_wizard.gif[Start the Virtual Media Wizard from the BMC UI.]
. Click *Browse* and select the compute firmware `.iso` file.
. Click *Connect*.
A popup indicating success is displayed, along with the path and device showing at the bottom. You can close the *Virtual Media* window.
+
image::virtual_med_popup.gif[Popup window showing ISO upload success.]
. Reboot the node by pressing *F12* and clicking *Restart* or clicking *Power Control > Set Power Reset*.
. During reboot, press *F11* to select the boot options and load the ISO.
. Select *AMI Virtual CDROM* from the list displayed and click *Enter*. If you do not see AMI Virtual CDROM in the list, go into the BIOS and enable it in the boot list. The node will reboot after you save. During the reboot, press *F11*.
+
image::boot_device.gif[Shows the window where you can select the boot device.]
. On the screen displayed, click *Enter*.
+
NOTE: Some of the firmware upgrades might cause the console to disconnect and/or cause your session on the BMC to disconnect. You can log back into the BMC, however some services, such as the console, might not be available due to the firmware upgrades. After the upgrades have completed, the node will perform a cold reboot, which can take approximately five minutes.

. If you get disconnected from the console, select *Remote Control* and click *Launch KVM* or *Launch KVM (Java)* to reconnect and verify when the node has finished booting back up. You might need multiple reconnects to verify that the node booted successfully.
+
CAUTION: During the powering on process, for approximately five minutes, the KVM console displays *No Signal*.

. After the node is powered on, select *Dashboard > Device Information > More info* to verify the BIOS and BMC versions. The upgraded BIOS and BMC versions are displayed. The upgraded version of the BIOS will not be displayed until the node has fully booted up.
. If you placed the node in maintenance mode, after the node boots to ESXi, right-click the host (compute node) name, and select *Maintenance Mode > Exit Maintenance Mode*, and migrate the VMs back to the host.
. In vCenter, with the host name selected, configure and verify the BIOS version.

[discrete]
== Find more information

* https://docs.netapp.com/hci/index.jsp[NetApp HCI Documentation Center^]
* https://docs.netapp.com/us-en/documentation/hci.aspx[NetApp HCI Resources Page^]
