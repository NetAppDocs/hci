---
sidebar: sidebar
permalink: docs/task_hcc_expand_compute_and_storage.html
summary: After you finish NetApp HCI deployment, you can expand and configure NetApp HCI storage resources by using NetApp Hybrid Cloud Control.
keywords: netapp, hci, on premise, expand, storage, compute, cluster
---

= Expand NetApp HCI storage and compute resources at the same time

:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

[.lead]
After you finish NetApp HCI deployment, you can expand and configure NetApp HCI storage and compute resources at the same time by using NetApp Hybrid Cloud Control.

.Before you begin
* Ensure that the vSphere instance of NetApp HCI is using vSphere Enterprise Plus licensing if you are expanding a deployment with Virtual Distributed Switches.
* Ensure that none of the vCenter or vSphere instances in use with NetApp HCI have expired licenses.
* Ensure that you have the vCenter administrator account credentials ready.
* Ensure that you have free and unused IPv4 addresses on the same network segment as existing nodes (each new node must be installed on the same network as existing nodes of its type).
* Ensure that you have one of the following types of SolidFire storage cluster accounts:
** The native administrator account that was created during initial deployment
** A custom user account with Cluster Admin, Drives, Volumes, and Nodes permissions
* Ensure that you have performed the following actions with each new node:
** Installed the new node in the NetApp HCI chassis by following the  link:task_hci_installhw.html[installation instructions].
** Cabled and powered on the new node
* Ensure that you have the management IPv4 address of an already installed storage node. You can find the IP address in the *NetApp Element Management* > *Cluster* > *Nodes* tab of the NetApp Element Plug-in for vCenter Server.
* Ensure that each new node uses the same network topology and cabling as the existing storage or compute clusters.

.About this task
* You can intermix the H410C compute node with existing NetApp HCI compute and storage nodes in the same chassis and cluster.
* You cannot intermix compute nodes and BPU-enabled compute nodes in the same cluster. If you select a GPU-enabled compute node, CPU-only compute nodes become unselectable, and vice versa.
* If you are adding compute nodes with CPU generations that are different than the CPU generation of the existing compute nodes and Enhanced vMotion Compatibility (EVC) is disabled on the controlling vCenter instance, you must enable EVC before proceeding. This ensures vMotion functionality after expansion is complete.

.Steps
. Open a web browser and browse to the IP address of the management node. For example:
+
----
https://[management node IP address]
----
. Log in to NetApp Hybrid Cloud Control by providing the NetApp HCI storage cluster administrator credentials.
. Click *Expand* at the top right corner of the interface.
+
The browser opens the NetApp Deployment Engine.
. Log in to the NetApp Deployment Engine by providing the NetApp HCI storage cluster administrator credentials.
. On the *Welcome* page, click *Yes* and click *Continue*.
. On the *End User License* page, read the VMware End User License Agreement and click *I accept* to accept the terms and click *Continue*.
. On the *vCenter* page, complete the following steps:
.. Enter a FQDN or IP address and administrator credentials for the vCenter instance associated with your NetApp HCI installation.
.. Click *Continue*.
.. Select a vSphere datacenter where you want to add the compute nodes, or click *Create New Datacenter* to add the compute nodes to a new datacenter.
+
NOTE: If you click Create New Datacenter, the Cluster field is automatically populated.

.. If you selected an existing datacenter, select a vSphere cluster with which the new compute nodes should be associated.
+
NOTE: If NetApp HCI cannot recognize the network settings of the cluster you have selected for expansion, ensure that the vmkernel and vmnic mapping for the management, storage and vMotion networks are set to the deployment defaults. See link:task_nde_supported_net_changes.html[supported networking changes] for more information.

.. Click *Continue*.
. On the *ESXi Credentials* page, enter an ESXi root password for the compute node or nodes you are adding.
+
You should use the same password that was created during the initial NetApp HCI deployment.
. Click *Continue*.
. If you created a new vSphere datacenter cluster, on the *Network Topology* page, select a network topology to match the new compute nodes you are adding.
+
NOTE: You an select the two-cable option only if your compute nodes are using the two-cable topology and the existing NetApp HCI deployment is configured with VLAN IDs.

. On the *Available Inventory* page, select the storage and compute nodes you want to add and click *Continue*.
+
IMPORTANT: For some compute nodes, you might need to enable EV at the highest level that your vCenter version supports before you can add them to your installation. You need to use the vSphere client to enable EVC for these compute nodes. After you enable it, refresh the Inventory page and try adding the compute nodes again.

. Click *Continue*.
. *Optional*: If you created a new vSphere datacenter cluster, on the *Network Settings* page, import network information from an existing NetApp HCI deployment by selecting the *Copy Setting from an Existing Cluster* checkbox.
+
This populates the default gateway and subnet information for each network.
. On the *Network Settings* page, some of the network information has been detected from the initial deployment. Each new storage node is listed by serial number, and you need to assign the new network information to it. For each new storage node, complete the following steps:
.. *Hostname*: If NetApp HCI detected a naming prefix, copy it from the Detected Naming Prefix field, and insert it as the prefix for the new unique hostname you add in the Hostname field.
.. *Management Address*: Enter a management IP address for the new storage node that is within the management network subnet.
.. *Storage (iSCSI) IP Address*: Enter an iSCSI IP address for the new storage node that is within the iSCSI network subnet.
.. Click *Continue*.
+
NOTE: NetApp HCI might take some time to validate the IP addresses you enter. The Continue button becomes available when IP address validation completes.

. On the *Review* page in the Network Settings section, new nodes are shown in the bold text. To make changes in any section, do the following:
.. Click *Edit* for that section.
.. After you finish, click *Continue* on any subsequent pages to return to the Review page.
. *Optional*: If you do not want to send cluster statistics and support information to NetApp hosted Active IQ servers, clear the final checkbox.
+
This disables real-time health and diagnostic monitoring for NetApp HCI. Disabling this feature removes the ability for NetApp to proactively support and monitor NetApp HCI to detect and resolve issues before production is impacted.
. Click *Add Nodes*.
+
You can monitor the progress while NetApp HCI adds and configures the resources.
. *Optional*: Verify that any new nodes are visible in the VMware vSphere Web Client (for compute nodes) or the Element Plug-in for vCenter Server (for storage nodes).
+
NOTE: If you expanded a two-node storage cluster to four nodes or more, the pair of Witness Nodes previously used by the storage cluster are still visible as standby virtual machines in vSphere. The newly expanded storage cluster does not use them; if you want to reclaim VM resources, you can link:task_hci_removewn.html[manually remove^] the Witness Node virtual machines.

[discrete]
== Find more information
* https://www.netapp.com/hybrid-cloud/hci-documentation/[NetApp HCI Resources Page^]
* https://docs.netapp.com/us-en/vcp/index.html[NetApp Element Plug-in for vCenter Server^]
* https://library.netapp.com/ecm/ecm_download_file/ECMLP2856176[NetApp HCI Compute and Storage Nodes Installation and Setup Instructions^]
* https://kb.vmware.com/s/article/1003212[VMware Knowledge Base: Enhanced vMotion Compatibility (EVC) processor support^]
