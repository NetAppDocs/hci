---
sidebar: sidebar
permalink: docs/task_upgrade_compute_prechecks.html
summary: As part of a NetApp HCI system upgrade, you need to run compute node health checks before performing firwamre upgrades.
keywords: netapp, compute prechecks, compute node upgrade, netapp hci
---

= Run compute node health checks prior to upgrading compute firmware

:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

[.lead]
You must run health checks prior to upgrading compute firmware to ensure all compute nodes in your cluster are ready to be upgraded. Compute node health checks can only be run against compute clusters of one or more managed NetApp HCI compute nodes.

.What you'll need

* You have updated to the latest management services bundle (2.11 or later).
* You are running management node 11.3 or later.
* Your storage cluster is running NetApp Element software 11.3 or later.

.Health check options

You can run health checks using NetApp Hybrid Cloud Control (HCC) UI or HCC API:

* <<Use NetApp Hybrid Cloud Control to run compute node health checks prior to upgrading firmware>> (Preferred method)
* <<Use API to run compute node health checks prior to upgrading firmware>>

You can also find out more about compute node health checks that are run by the service:

* <<Compute node health checks made by the service>>

== Use NetApp Hybrid Cloud Control to run compute node health checks prior to upgrading firmware

Using NetApp Hybrid Cloud Control (HCC), you can verify that a compute node is ready for a firmware upgrade.

.Steps

. Open a web browser and browse to the IP address of the management node:
+
----
https://<ManagementNodeIP>/hcc
----
. Log in to NetApp Hybrid Cloud Control by providing the storage cluster administrator credentials.
. Click *Upgrade* near the top right of the interface.
. On the *Upgrades* page, select the *Compute firmware* tab.
. Click the health check image:hcc_healthcheck_icon.png[icon] for the cluster you want to check for upgrade readiness.
. On the *Compute Health Check* page, click *Run Health Check*.
. If there are issues, the page provides a report. Do the following:
.. Go to the specific KB article listed for each issue or perform the specified remedy.
.. If a KB is specified, complete the process described in the relevant KB article.
.. After you have resolved cluster issues, click *Re-Run Health Check*.

After the health check completes without errors, the compute nodes in the cluster are ready to upgrade. See  link:task_hcc_upgrade_compute_node_firmware.html[Update compute node firmware] to proceed.

== Use API to run compute node health checks prior to upgrading firmware

You can use REST API to verify that compute nodes in a cluster are ready to be upgraded. The health check verifies that there are no obstacles to upgrading, such as ESXi host issues or other vSphere issues. You will need to run compute node health checks for each compute cluster in your environment.

.Steps

. Locate the controller ID and cluster ID:
.. Open the inventory service REST API UI on the management node:
+
----
https://[management node IP]/inventory/1/
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client` if the value is not already populated.
... Click *Authorize* to begin a session.
.. From the REST API UI, click *GET ​/installations*.
.. Click *Try it out*.
.. Click *Execute*.
.. From the code 200 response body, copy the `"id"` for the installation you plan to use for health checks.
.. From the REST API UI, click *GET ​/installations​/{id}*.
.. Click *Try it out*.
.. Enter the installation ID.
.. Click *Execute*.
.. From the code 200 response body, copy the IDs for each of the following:
... The cluster ID (`"clusterID"`)
... A controller ID (`"controllerId"`)
+
----
{
  "_links": {
    "collection": "https://10.117.187.199/inventory/1/installations",
    "self": "https://10.117.187.199/inventory/1/installations/xx94f6f0-12a6-412f-8b5e-4cf2z58329x0"
  },
  "compute": {
    "errors": [],
    "inventory": {
      "clusters": [
        {
          "clusterId": "domain-1",
          "controllerId": "abc12c3a-aa87-4e33-9f94-xx588c2cdcf6",
          "datacenterName": "NetApp-HCI-Datacenter-01",
          "installationId": "xx94f6f0-12a6-412f-8b5e-4cf2z58329x0",
          "installationName": "test-nde-mnode",
          "inventoryType": "managed",
          "name": "NetApp-HCI-Cluster-01",
          "summary": {
            "nodeCount": 2,
            "virtualMachineCount": 2
          }
        }
      ],
----
. Run health checks on the compute nodes in the cluster:
.. Open the compute service REST API UI on the management node:
+
----
https://[management node IP]/vcenter/1/
----
.. Click *Authorize* and complete the following:
... Enter the cluster user name and password.
... Enter the client ID as `mnode-client` if the value is not already populated.
... Click *Authorize* to begin a session.
.. Click *POST /compute​/{CONTROLLER_ID}​/health-checks*.
.. Click *Try it out*.
.. Enter the `"controllerId"` you copied from the previous step in the *Controller_ID* parameter field.
.. In the payload, enter the `"clusterId"` that you copied from the previous step as the `"cluster"` value and remove the `"nodes"` parameter.
+
----
{
  "cluster": "domain-1"
}
----
.. Click *Execute* to run a health check on the cluster.
+
The code 200 response gives a `"resourceLink"` URL with the task ID appended that is needed to confirm the health check results.
+
----
{
  "resourceLink": "https://10.117.150.84/vcenter/1/compute/tasks/[This is the task ID for health check task results]",
  "serviceName": "vcenter-v2-svc",
  "taskId": "ab12c345-06f7-42d7-b87c-7x64x56x321x",
  "taskName": "VCenter service health checks"
}
----
.. Copy the task ID portion of the `"resourceLink"` URL to verify the task result.
. Verify the result of the health checks:
.. Return to the compute service REST API UI on the management node:
+
----
https://[management node IP]/vcenter/1/
----
.. Click *GET /compute​/tasks/{task_id}*.
.. Click *Try it out*.
.. Enter the task ID portion of the `"resourceLink"` URL from the *POST /compute​/{CONTROLLER_ID}​/health-checks* code 200 response in the `task_id` parameter field.
.. Click *Execute*.
.. If the `status` returned indicates that there were problems regarding compute node health, do the following:
... Go to the specific KB article (`KbLink`) listed for each issue or perform the specified remedy.
... If a KB is specified, complete the process described in the relevant KB article.
... After you have resolved cluster issues, run *POST /compute​/{CONTROLLER_ID}​/health-checks* again (see step 2).

If health checks complete without issues, the response code 200 indicates a successful result.

== Compute node health checks made by the service
Compute health checks, whether performed by HCC or API methods, make the following checks per node. Depending on your environment, some of these checks might be skipped. You should re-run health checks after resolving any detected issues.
|===
| Check description | Node/cluster | Action needed to resolve | Knowledgebase article with procedure

| Is DRS enabled and fully automated? | Cluster | Turn on DRS and make sure it is fully automated. | link:https://kb.netapp.com/Advice_and_Troubleshooting/Data_Storage_Software/Virtual_Storage_Console_for_VMware_vSphere/How_to_enable_DRS_in_vSphere[See this KB]. NOTE: If you have standard licensing, put the ESXi host into maintenance mode and ignore this health check failure warning.

| Is DPM disabled in vSphere? | Cluster | Turn off Distributed Power Management. | link:https://kb.netapp.com/Advice_and_Troubleshooting/Data_Storage_Software/Element_Plug-in_for_vCenter_server/How_to_disable_DPM_in_VMware_vCenter[See this KB].

| Is HA admission control disabled in vSphere? | Cluster | Turn off HA admission control. | link:https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/How_to_control_enable_HA_admission_in_vSphere[See this KB].

| Is FT enabled for a VM on a host in the cluster? | Node |  	Suspend Fault Tolerance on any affected virtual machines. | link:https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/How_to_suspend_fault_tolerance_on_virtual_machines_in_a_vSphere_cluster[See this KB].

| Are there critical alarms in vCenter for the cluster? | Cluster | Launch vSphere and resolve and/or acknowledge any alerts before proceeding. | No KB needed to resolve issue.

| Are there generic/global informational alerts in vCenter? | Cluster |  	Launch vSphere and resolve and/or acknowledge any alerts before proceeding. | No KB needed to resolve issue.

| Are management services up to date? | HCI system | You must update management services before you perform an upgrade or run pre-upgrade health checks. | No KB needed to resolve issue. See link:task_hcc_update_management_services.html[this article] for more information.

| Are there errors on the current ESXi node in vSphere? | Node | Launch vSphere and resolve and/or acknowledge any alerts before proceeding. | No KB needed to resolve issue.

| Is virtual media mounted to a VM on a host in the cluster? | Node | Unmount all virtual media disks (CD/DVD/floppy) from the VMs. | No KB needed to resolve issue.

| Is BMC version the minimum required version that has RedFish support? | Node | Manually update your BMC firmware. | No KB needed to resolve issue.

| Is ESXi host up and running? | Node | Start your ESXi host. | No KB needed to resolve issue.

| Do any virtual machines reside on local ESXi storage? | Node/VM | Remove or migrate local storage attached to virtual machines. | No KB needed to resolve issue.

| Is BMC up and running? | Node | Power on your BMC and ensure it is connected to a network this management node can reach. | No KB needed to resolve issue.

| Are there partner ESXi host(s) available? | Node | Make one or more ESXi host(s) in cluster available (not in maintenance mode) to migrate virtual machines. | No KB needed to resolve issue.

| Are you able to connect with BMC via IPMI protocol? | Node | Enable IPMI protocol on Baseboard Management Controller (BMC). | No KB needed to resolve issue.

| Is ESXi host mapped to hardware host (BMC) correctly? | Node | The ESXi host is not mapped to the Baseboard Management Controller (BMC) correctly. Correct the mapping between ESXi host and hardware host. | No KB needed to resolve issue. See link:task_hcc_edit_bmc_info.html[this article] for more information.

| What is the status of the Witness Nodes in the cluster? None of the witness nodes identified are up and running. | Node | A Witness Node is not running on an alternate ESXi host. Power on the Witness Node on an alternate ESXi host and re-run the health check. *One Witness Node must be running in the HCI installation at all times*. | https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/How_to_resolve_witness_node_issues_prior_to_upgrading_compute_nodes[See this KB]

| What is the status of the Witness Nodes in the cluster? The witness node is up and running on this ESXi host and the alternate witness node is not up and running. | Node | A Witness Node is not running on an alternate ESXi host. Power on the Witness Node on an alternate ESXi host. When you are ready to upgrade this ESXi host, shut down the witness node running on this ESXi host and re-run the health check. *One Witness Node must be running in the HCI installation at all times*. | https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/How_to_resolve_witness_node_issues_prior_to_upgrading_compute_nodes[See this KB]

| What is the status of the Witness Nodes in the cluster? Witness node is up and running on this ESXi host and the alternate node is up but is running on the same ESXi host. | Node | Both Witness Nodes are running on this ESXi host. Relocate one Witness Node to an alternate ESXi host. When you are ready to upgrade this ESXi host, shut down the Witness Node remaining on this ESXi host and re-run the health check. *One Witness Node must be running in the HCI installation at all times*. | https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/How_to_resolve_witness_node_issues_prior_to_upgrading_compute_nodes[See this KB]

| What is the status of the Witness Nodes in the cluster? Witness node is up and running on this ESXi host and the alternate witness node is up and running on another ESXi host. | Node | A Witness Node is running locally on this ESXi host. When you are ready to upgrade this ESXi host, shut down the Witness Node only on this ESXi host and re-run the health check. *One Witness Node must be running in the HCI installation at all times*. | https://kb.netapp.com/Advice_and_Troubleshooting/Hybrid_Cloud_Infrastructure/NetApp_HCI/How_to_resolve_witness_node_issues_prior_to_upgrading_compute_nodes[See this KB]

|===

[discrete]
== Find more information

* https://docs.netapp.com/us-en/vcp/index.html[NetApp Element Plug-in for vCenter Server^]
* https://www.netapp.com/hybrid-cloud/hci-documentation/[NetApp HCI Resources Page^]
